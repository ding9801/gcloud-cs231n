{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de CNN para CIFAR10\n",
    "Um modelo de **Convolutional Neural Network** construído no *TensorFlow* treinado para classificação de imagens usando a base de dados **CIFAR10**. \n",
    "\n",
    "## Carregando Bibliotecas e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo a arquitetura\n",
    "A arquitetura escolhida para o modelo foi:\n",
    "\n",
    "* **Convolutional Neural Network** - 32 layers, 3x3, 1 stride, pad \"SAME\"\n",
    "* **BatchNorm** - Spatial BN\n",
    "* **Convolutional Neural Network** - 64 layers, 3x3, 1 stride, pad \"SAME\"\n",
    "* **BatchNorm** - Spatial BN\n",
    "* **2x2 MaxPool** - 2 stride, 2 ksize\n",
    "* **Convolutional Neural Network** - 128 layers, 3x3, 1 stride, pad \"SAME\"\n",
    "* **BatchNorm** - Spatial BN\n",
    "* **FullyConnected** - FC Layer, 4096 neurons\n",
    "* **BatchNorm** - Vanilla BN\n",
    "* **Dropout** - Dropout com keep_prob: 50%\n",
    "* **FullyConnectedLayer** - FC Layer, 10 neurons\n",
    "* **Softmax** - Cálculo das probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "# Construindo o modelo\n",
    "def model(X, y, kprob,is_training):\n",
    "    \n",
    "    with tf.name_scope(\"ConvLayer1\"):\n",
    "        W_conv1 = tf.get_variable(\"W_conv1\", shape=[3, 3, 3, 32])\n",
    "        b_conv1 = tf.get_variable(\"b_conv1\", shape=[32])\n",
    "        \n",
    "        a1 = tf.nn.conv2d(X, W_conv1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv1\n",
    "        h_conv1 = tf.nn.relu(a1)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W_conv1)\n",
    "        tf.summary.histogram(\"biases\", b_conv1)\n",
    "        tf.summary.histogram(\"activations\", h_conv1)\n",
    "        \n",
    "    with tf.name_scope(\"BatchNorm1\"):\n",
    "        h_bn1 = tf.layers.batch_normalization(h_conv1, axis=1, training=is_training)\n",
    "        tf.summary.histogram(\"normalized\", h_bn1)\n",
    "    \n",
    "    with tf.name_scope(\"ConvLayer2\"):\n",
    "        W_conv2 = tf.get_variable(\"W_conv2\", shape=[3, 3, 32, 64])\n",
    "        b_conv2 = tf.get_variable(\"b_conv2\", shape=[64])\n",
    "        \n",
    "        a2 = tf.nn.conv2d(h_bn1, W_conv2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv2\n",
    "        h_conv2 = tf.nn.relu(a2)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W_conv2)\n",
    "        tf.summary.histogram(\"biases\", b_conv2)\n",
    "        tf.summary.histogram(\"activations\", h_conv2)\n",
    "        \n",
    "    with tf.name_scope(\"BatchNorm2\"):\n",
    "        h_bn2 = tf.layers.batch_normalization(h_conv2, axis=1, training=is_training)\n",
    "        tf.summary.histogram(\"normalized\", h_bn2)\n",
    "        \n",
    "    with tf.name_scope(\"max-pool\"):\n",
    "        h_pool = tf.nn.max_pool(h_bn2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "        \n",
    "    with tf.name_scope(\"ConvLayer3\"):\n",
    "        W_conv3 = tf.get_variable(\"W_conv3\", shape=[3, 3, 64, 128])\n",
    "        b_conv3 = tf.get_variable(\"b_conv3\", shape=[128])\n",
    "        \n",
    "        a3 = tf.nn.conv2d(h_pool, W_conv3, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv3\n",
    "        h_conv3 = tf.nn.relu(a3)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W_conv3)\n",
    "        tf.summary.histogram(\"biases\", b_conv3)\n",
    "        tf.summary.histogram(\"activations\", h_conv3)\n",
    "        \n",
    "    with tf.name_scope(\"BatchNorm3\"):\n",
    "        h_bn3 = tf.layers.batch_normalization(h_conv3, axis=1, training=is_training)\n",
    "        tf.summary.histogram(\"normalized\", h_bn3)\n",
    "        \n",
    "    with tf.name_scope(\"FullyConnected\"):\n",
    "        h_reshaped = tf.reshape(h_bn3, [-1, 16*16*128])\n",
    "        \n",
    "        W1 = tf.get_variable(\"W1\", shape=[16*16*128, 4096])\n",
    "        b1 = tf.get_variable(\"b1\", shape=[4096])\n",
    "        \n",
    "        a4 = tf.matmul(h_reshaped, W1) + b1\n",
    "        h_fc1 = tf.nn.relu(a4)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W1)\n",
    "        tf.summary.histogram(\"biases\", b1)\n",
    "        tf.summary.histogram(\"activations\", h_fc1)\n",
    "    \n",
    "    with tf.name_scope(\"VanillaBatchNorm\"):\n",
    "        h_bn4 = tf.layers.batch_normalization(h_fc1, axis=1, training=is_training)\n",
    "        tf.summary.histogram(\"normalized\", h_bn4)\n",
    "        \n",
    "    with tf.name_scope(\"Dropout\"):\n",
    "        h_drop = tf.nn.dropout(h_bn4, keep_prob=kprob)\n",
    "    \n",
    "    with tf.name_scope(\"FullyConnectedOut\"):\n",
    "        W2 = tf.get_variable(\"W2\", shape=[4096, 10])\n",
    "        b2 = tf.get_variable(\"b2\", shape=[10])\n",
    "        \n",
    "        scores = tf.matmul(h_drop, W2) + b2\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W2)\n",
    "        tf.summary.histogram(\"biases\", b2)\n",
    "        tf.summary.histogram(\"scores\", scores)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X-input\")\n",
    "y = tf.placeholder(tf.int64, [None], name=\"y-input\")\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "kprob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Decay Learning rate com step, utilizando os intervalos de iterações 7000 e 12500\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "initial_lr = tf.placeholder(tf.float32)\n",
    "values = [initial_lr, initial_lr*0.5, initial_lr*0.25]\n",
    "boundaries = [7000, 12500]\n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "\n",
    "y_out = model(X, y, kprob, is_training)\n",
    "with tf.name_scope(\"cross-entropy\"):\n",
    "    mean_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, 10), logits=y_out))\n",
    "    tf.summary.scalar(\"cross-entropy\", mean_loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando se **y_out** tem o shape esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 779 ms per loop\n",
      "(64, 10)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(64, 32, 32,3)\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True,kprob:0.5,initial_lr: 5e-4})\n",
    "        %timeit sess.run(y_out,feed_dict={X:x,is_training:True,kprob:0.5,initial_lr:5e-4})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo hiper-parâmetros\n",
    "Definição de hiper-parâmetros, primeiro a maneira *coarse*, ou seja, bruta, testando um maior *range* nos valores a serem otimizados. \n",
    "\n",
    "Os hiper-parâmetros testados serão o **learning rate** inicial e a **probabilidade do dropout**. \n",
    "\n",
    "### Coarse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 0:\n",
      "\tLearning Rate: 1.09e-04 - Dropout Parameter: 0.45\n",
      "\tTraining Accuracy: 48.44%\n",
      "\tValidation Accuracy: 52.70%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 1:\n",
      "\tLearning Rate: 1.97e-04 - Dropout Parameter: 0.76\n",
      "\tTraining Accuracy: 51.56%\n",
      "\tValidation Accuracy: 51.80%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 2:\n",
      "\tLearning Rate: 1.47e-06 - Dropout Parameter: 0.57\n",
      "\tTraining Accuracy: 40.62%\n",
      "\tValidation Accuracy: 41.70%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 3:\n",
      "\tLearning Rate: 3.12e-05 - Dropout Parameter: 0.56\n",
      "\tTraining Accuracy: 53.12%\n",
      "\tValidation Accuracy: 56.50%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 4:\n",
      "\tLearning Rate: 1.64e-04 - Dropout Parameter: 0.54\n",
      "\tTraining Accuracy: 35.94%\n",
      "\tValidation Accuracy: 56.40%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 5:\n",
      "\tLearning Rate: 7.36e-04 - Dropout Parameter: 0.72\n",
      "\tTraining Accuracy: 48.44%\n",
      "\tValidation Accuracy: 48.60%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 6:\n",
      "\tLearning Rate: 1.01e-04 - Dropout Parameter: 0.82\n",
      "\tTraining Accuracy: 51.56%\n",
      "\tValidation Accuracy: 54.90%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 7:\n",
      "\tLearning Rate: 3.95e-05 - Dropout Parameter: 0.84\n",
      "\tTraining Accuracy: 65.62%\n",
      "\tValidation Accuracy: 54.40%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 8:\n",
      "\tLearning Rate: 2.26e-06 - Dropout Parameter: 0.75\n",
      "\tTraining Accuracy: 40.62%\n",
      "\tValidation Accuracy: 49.00%\n",
      "0 - 100 - 200 - 300 - 400 - \n",
      "Iteration 9:\n",
      "\tLearning Rate: 4.15e-04 - Dropout Parameter: 0.77\n",
      "\tTraining Accuracy: 50.00%\n",
      "\tValidation Accuracy: 54.00%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for i in range(10):\n",
    "    # Definindo os parâmetros\n",
    "    learning_rate = 10**np.random.uniform(-6, -3)\n",
    "    keep_p = np.random.uniform(0.25, 0.85)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Iniciando as variáveis\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Fazendo a predição\n",
    "        correct_prediction = tf.equal(tf.argmax(y_out,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        # Embaralhando os índices\n",
    "        train_indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(train_indices)\n",
    "        \n",
    "        # Sempre estará treinando\n",
    "        training_now = True\n",
    "        \n",
    "        # Configurando as variáveis que desejamos avaliar\n",
    "        variables = c\n",
    "        \n",
    "        for k in range(401):\n",
    "            # Criando os batches\n",
    "            start_idx = (k * batch_size) % X_train.shape[0]\n",
    "            idx = train_indices[start_idx:start_idx+batch_size]\n",
    "\n",
    "            # Criando o dicionário a ser passado\n",
    "            feed_dict = {X: X_train[idx, :],\n",
    "                        y: y_train[idx],\n",
    "                        is_training: training_now,\n",
    "                        kprob: keep_p,\n",
    "                        initial_lr: learning_rate}\n",
    "\n",
    "            # Chamando o método\n",
    "            train_loss, train_acc, corr, _ = sess.run(variables, feed_dict=feed_dict)\n",
    "            \n",
    "            if (k % 100 == 0):\n",
    "                print(\"%d\" % k, end=\" - \")\n",
    "        # Verificando o validation e training acc\n",
    "        val_dict = {X: X_val,\n",
    "                   y: y_val,\n",
    "                   is_training: False,\n",
    "                   kprob: 1.0,\n",
    "                   initial_lr: learning_rate}\n",
    "        \n",
    "        val_loss, val_acc, corr, _ = sess.run(variables, feed_dict=val_dict)\n",
    "        \n",
    "        print(\"\\nIteration %d:\\n\\tLearning Rate: %.2e - Dropout Parameter: %.2f\" % (i, learning_rate, keep_p))\n",
    "        print(\"\\tTraining Accuracy: %.2f%%\" % (100*train_acc))\n",
    "        print(\"\\tValidation Accuracy: %.2f%%\" % (100*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine\n",
    "Aplicando 1 epoch em cada um dos ranges que tiveram melhor desempenho na **coarse**.\n",
    "\n",
    "* **Learning Rate**: -3 a -5\n",
    "* **Dropout Parameter**: 0.45 a 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 0:\n",
      "\tLearning Rate: 8.20e-05 - Dropout Parameter: 0.56\n",
      "\tTraining Accuracy: 75.00%\n",
      "\tValidation Accuracy: 66.00%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 1:\n",
      "\tLearning Rate: 5.43e-05 - Dropout Parameter: 0.61\n",
      "\tTraining Accuracy: 60.00%\n",
      "\tValidation Accuracy: 59.70%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 2:\n",
      "\tLearning Rate: 2.88e-04 - Dropout Parameter: 0.64\n",
      "\tTraining Accuracy: 67.50%\n",
      "\tValidation Accuracy: 66.90%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 3:\n",
      "\tLearning Rate: 8.61e-04 - Dropout Parameter: 0.62\n",
      "\tTraining Accuracy: 62.50%\n",
      "\tValidation Accuracy: 59.80%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 4:\n",
      "\tLearning Rate: 2.47e-05 - Dropout Parameter: 0.62\n",
      "\tTraining Accuracy: 82.50%\n",
      "\tValidation Accuracy: 60.30%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 5:\n",
      "\tLearning Rate: 6.64e-05 - Dropout Parameter: 0.62\n",
      "\tTraining Accuracy: 77.50%\n",
      "\tValidation Accuracy: 63.80%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 6:\n",
      "\tLearning Rate: 9.60e-05 - Dropout Parameter: 0.46\n",
      "\tTraining Accuracy: 72.50%\n",
      "\tValidation Accuracy: 67.80%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 7:\n",
      "\tLearning Rate: 8.20e-04 - Dropout Parameter: 0.50\n",
      "\tTraining Accuracy: 75.00%\n",
      "\tValidation Accuracy: 63.70%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 8:\n",
      "\tLearning Rate: 5.81e-05 - Dropout Parameter: 0.48\n",
      "\tTraining Accuracy: 60.00%\n",
      "\tValidation Accuracy: 66.60%\n",
      "0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - 0 - 100 - 200 - 300 - 400 - 500 - 600 - 700 - \n",
      "Iteration 9:\n",
      "\tLearning Rate: 7.98e-04 - Dropout Parameter: 0.68\n",
      "\tTraining Accuracy: 77.50%\n",
      "\tValidation Accuracy: 61.60%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "for i in range(10): \n",
    "    ## Definindo os parâmetros\n",
    "    learning_rate = 10**np.random.uniform(-3, -5)\n",
    "    dropout_param = np.random.uniform(0.45, 0.7)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Iniciando as variáveis\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Fazendo a predição\n",
    "        correct_prediction = tf.equal(tf.argmax(y_out, 1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, np.float32))\n",
    "        \n",
    "        # Embaralhando os índices\n",
    "        train_indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(train_indices)\n",
    "        \n",
    "        # Sempre estará treinando\n",
    "        training = True\n",
    "        \n",
    "        # Configurando as variáveis que desejamos avaliar\n",
    "        variables = [mean_loss, accuracy, correct_prediction, train_step]\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for k in range(int(math.ceil(X_train.shape[0]/batch_size))):\n",
    "                # Criando os batches\n",
    "                start_idx = (k * batch_size) % X_train.shape[0]\n",
    "                idx = train_indices[start_idx:start_idx+batch_size]\n",
    "                \n",
    "                # Criando o dicionário para alimentar o modelo\n",
    "                feed_dict = {X: X_train[idx, :],\n",
    "                            y: y_train[idx],\n",
    "                            is_training: training,\n",
    "                            kprob: dropout_param,\n",
    "                            initial_lr: learning_rate }\n",
    "                \n",
    "                # Chamando o método\n",
    "                train_loss, train_acc, corr, _ = sess.run(variables, feed_dict=feed_dict)\n",
    "                \n",
    "                if (k % 100 == 0):\n",
    "                    print(\"%d\" % k, end=\" - \")\n",
    "                    \n",
    "        # Verificando o validation acc\n",
    "        val_dict = { X: X_val,\n",
    "                    y: y_val,\n",
    "                    is_training: False,\n",
    "                    kprob: 1.0,\n",
    "                    initial_lr: learning_rate }\n",
    "        val_loss, val_acc, corr, _ = sess.run(variables, feed_dict=val_dict)\n",
    "        \n",
    "        print(\"\\nIteration %d:\\n\\tLearning Rate: %.2e - Dropout Parameter: %.2f\" % (i, learning_rate, dropout_param))\n",
    "        print(\"\\tTraining Accuracy: %.2f%%\" % (100*train_acc))\n",
    "        print(\"\\tValidation Accuracy: %.2f%%\" % (100*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo final\n",
    "\n",
    "O modelo final pode ser definido a partir da busca por hiperparâmetros (*coarse to fine*). Pelas visão da busca *fine*, temos que o **learning rate** de 2.88e-4 e o **dropout parameter** de 0.64, trouxeram um desempenho no conjunto de validação de 66.90% e no conjunto de testes de 67.50%, o que mostra um bom desempenho (o segundo melhor entre os 10 testados no conjunto de validação), e uma aproximação grande entre desempenho de testes e de validação, o que mostra que o modelo pode ter sua capacidade aumentada antes de atingir *overfitting*. \n",
    "\n",
    "O melhor desempenho foi atingido pelo **learning rate** de 9.6e-5 e um **dropout parameter** de 0.46, que teve um desempenho de 67.80% no conjunto de validação e 72.50% no conjunto de testes. O *gap* entre os dois continua pequeno, mostrando que o conjunto ainda pode ter sua capacidade aumentado antes de ating *overfitting*. \n",
    "\n",
    "Tendo esses dois modelos em mãos, vamos fazer o teste do segundo modelo. Como temos um **dropout parameter** razoavelmente alto (próximo de 0.5), podemos aumentar o número de *epochs*. Também será feito as devidas modificações ao fim de fazer a análise do modelo utilizando a ferramenta **Tensor Board** disponibilizada pelo framework *TensorFlow*.\n",
    "\n",
    "* **Learning Rate**: 3e-4\n",
    "* **Dropout Parameter**: 0.6\n",
    "* **Epochs**: 20\n",
    "* **Batch Size**: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "drop_param = 0.6\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "with tf.Session() as sess1:\n",
    "    # TensorBoard\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('/tmp/tensorflow/cifar10/3')\n",
    "    train_writer.add_graph(sess1.graph)\n",
    "    \n",
    "    # Inicializando todas as variáveis\n",
    "    sess1.run(tf.global_variables_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope(\"correct-prediction\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_out, 1), y)\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, np.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    # Embaralhando os índices\n",
    "    train_idx = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(train_idx)\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(\"Epoch %d\" % (e+1))\n",
    "        # Calculando quantas iterações existem em uma época\n",
    "        it_for_epoch = int(math.ceil(X_train.shape[0]/batch_size))\n",
    "        # Rodando por uma época\n",
    "        for i in range(it_for_epoch):\n",
    "            # Gerando índices para o minibatch\n",
    "            start_idx = (i*batch_size) % X_train.shape[0]\n",
    "            idx = train_idx[start_idx:start_idx+batch_size]\n",
    "\n",
    "            # Gerando o feed_dict\n",
    "            feed_dict = {X: X_train[idx, :],\n",
    "                         y: y_train[idx],\n",
    "                         is_training: True,\n",
    "                         kprob: drop_param,\n",
    "                         initial_lr: lr }\n",
    "\n",
    "            # Tamanho real do batch size\n",
    "            actual_batch_size = y_train[idx].shape[0]\n",
    "\n",
    "            # Calculando\n",
    "            if (i % 100 == 0):\n",
    "                summary_train, train_acc, _ = sess1.run([merged, accuracy, train_step], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary_train, i)\n",
    "                \n",
    "                val_dict = {X: X_val,\n",
    "                            y: y_val,\n",
    "                            is_training:False,\n",
    "                            kprob: 1.0,\n",
    "                            initial_lr: 0 }\n",
    "                summary_val, val_acc = sess1.run([merged, accuracy], feed_dict = val_dict)\n",
    "                train_writer.add_summary(summary_val, i)\n",
    "                print(\"\\tStep %d - Train Acc: %.2f%% -- Val Acc: %.2f%%\" % (i, 100*train_acc, 100*val_acc))\n",
    "            else:\n",
    "                _ = sess1.run(train_step, feed_dict=feed_dict)\n",
    "            \n",
    "        \n",
    "    train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
